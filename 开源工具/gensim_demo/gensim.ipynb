{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "texts = [['human', 'interface', 'computer'],\n",
    "['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "['eps', 'user', 'interface', 'system'],\n",
    "['system', 'human', 'system', 'eps'],\n",
    "['user', 'response', 'time'],\n",
    "['trees'],\n",
    "['graph', 'trees'],\n",
    "['graph', 'minors', 'trees'],\n",
    "['graph', 'minors', 'survey']]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemsView(<gensim.corpora.dictionary.Dictionary object at 0x000001D45A10EB70>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=9, num_nnz=28)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.70710678118654757), (3, 0.70710678118654757)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dow=[(0,1),(3,1)]\n",
    "tfidf[doc_dow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import jieba,re,os\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "def SplitSentence(inputfile,fout):      #语料处理：去符号，分词\n",
    "    fin = open(inputfile,'r')\n",
    "    for line in fin:\n",
    "        line1 = line.strip().decode('utf-8','ignore')\n",
    "        line2 = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*“”➕「」《》（）]+\".decode(\"utf-8\"), \"\".decode(\"utf-8\"), line1)\n",
    "        wordlist = list(jieba.cut(line2))\n",
    "        outstr = ''\n",
    "        for word in wordlist:\n",
    "            outstr += word\n",
    "            outstr += ' '\n",
    "        fout.write(outstr.strip().encode('utf-8')+'\\n')\n",
    "    fin.close()\n",
    "\n",
    "def TextLoader(dir):\n",
    "    fout = open('SplitSentence.txt', 'w')\n",
    "    for root, dirs, files in os.walk(dir):      # 遍历所有文件夹\n",
    "        for file in files:\n",
    "            inputfile = os.path.join(root, file)\n",
    "            print(inputfile)     #打印文件记录\n",
    "            SplitSentence(inputfile,fout)   #处理该文件\n",
    "    fout.close()\n",
    "    \n",
    "def Training():\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)   #打印日志\n",
    "    sentences = word2vec.Text8Corpus('SplitSentence.txt')  # 加载语料\n",
    "    model = word2vec.Word2Vec(sentences)  # 训练skip-gram模型; 默认参数\n",
    "    model.save('vector.bin')\n",
    "#model = word2vec.Word2Vec(sentences, sg=1, size=100, window=5, min_count=5, negative=3, sample=0.001, hs=1, workers=4)\n",
    "#\n",
    "# sentences：可以是一个list，对于大语料集，建议使用BrownCorpus,Text8Corpus或·ineSentence构建。\n",
    "#\n",
    "# sg：用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法,对低频词敏感；默认sg=0为CBOW算法。\n",
    "#\n",
    "# size是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为几十到几百之间。默认为100\n",
    "#\n",
    "# window是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）。\n",
    "#\n",
    "# min_count是对词进行过滤，频率小于min-count的单词则会被忽视，默认值为5。\n",
    "#\n",
    "# negative和sample可根据训练结果进行微调，sample表示更高频率的词被随机下采样到所设置的阈值，默认值为1e-3。\n",
    "#\n",
    "# hs=1表示层级softmax将会被使用，默认hs=0且negative不为0，则负采样将会被选择使用。\n",
    "#\n",
    "# workers控制训练的并行，此参数只有在安装了Cpython后才有效，否则只能使用单核。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
