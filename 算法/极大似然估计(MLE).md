

了解极大似然估计前先了解什么是贝叶斯决策

#### 贝叶斯决策

首先来看贝叶斯分类，我们都知道经典的贝叶斯公式：

![](F:\git\nlp\算法\images\极大似然估计\20170528002022807.jpg)

  其中：p(w)：为先验概率，表示每种类别分布的概率；![](F:\git\nlp\算法\images\极大似然估计\20170528002108539.jpg)：类条件概率，表示在某种类别前提下，某事发生的概率；而![](F:\git\nlp\算法\images\极大似然估计\20170528002145196.jpg)为后验概率，表示某事发生了，并且它属于某一类别的概率，有了这个后验概率，我们就可以对样本进行分类。后验概率越大，说明某事物属于这个类别的可能性越大，我们越有理由把它归到这个类别下。

​        我们来看一个直观的例子：**已知：**在夏季，某公园男性穿凉鞋的概率为1/2，女性穿凉鞋的概率为2/3，并且该公园中男女比例通常为2:1，**问题：**若你在公园中随机遇到一个穿凉鞋的人，请问他的性别为男性或女性的概率分别为多少？

​        从问题看，就是上面讲的，某事发生了，它属于某一类别的概率是多少？即后验概率。

 设：![](F:\git\nlp\算法\images\极大似然估计\20170528002248527.jpg)

由已知可得：

![](F:\git\nlp\算法\images\极大似然估计\20170528002344387.jpg)

  男性和女性穿凉鞋相互独立，所以![](F:\git\nlp\算法\images\极大似然估计\20170528002436496.jpg)

（若只考虑分类问题，只需要比较后验概率的大小，取值并不重要）。

由贝叶斯公式算出：![](F:\git\nlp\算法\images\极大似然估计\20170528002504950.jpg)

#### 问题引出

​        但是在实际问题中并不都是这样幸运的，我们能获得的数据可能只有有限数目的样本数据，而先验概率![img](https://img-blog.csdn.net/20170528002627998)和类条件概率(各类的总体分布)![img](https://img-blog.csdn.net/20170528002633154)都是未知的。根据仅有的样本数据进行分类时，一种可行的办法是我们需要先对先验概率和类条件概率进行估计，然后再套用贝叶斯分类器。

​        先验概率的估计较简单，1、每个样本所属的自然状态都是已知的（有监督学习）；2、依靠经验；3、用训练样本中各类出现的频率估计。

​        类条件概率的估计（非常难），原因包括：概率密度函数包含了一个随机变量的全部信息；样本数据可能不多；特征向量x的维度可能很大等等。总之要直接估计类条件概率的密度函数很难。解决的办法就是，把估计完全未知的概率密度![img](https://img-blog.csdn.net/20170528002633154)转化为估计参数。这里就将概率密度估计问题转化为参数估计问题，极大似然估计就是一种参数估计方法。当然了，概率密度函数的选取很重要，模型正确，在样本区域无穷时，我们会得到较准确的估计值，如果模型都错了，那估计半天的参数，肯定也没啥意义了。

 

#### 重要前提

​        上面说到，参数估计问题只是实际问题求解过程中的一种简化方法（由于直接估计类条件概率密度函数很困难）。所以能够使用极大似然估计方法的样本必须需要满足一些前提假设。

​        重要前提：训练样本的分布能代表样本的真实分布。每个样本集中的样本都是所谓独立同分布的随机变量 (iid条件)，且有充分的训练样本。

 



#### 1. 什么是极大似然估计

  在日常生活中，我们很容易无意中就使用到极大似然估计的思想，只是我们并不知道极大似然估计在数学中的如何确定以及推导的。下面我们使用两个例子让大家大概了解一下什么是**极大似然估计**：

（1）猎人师傅和徒弟一同去打猎，遇到一只兔子，师傅和徒弟同时放枪，兔子被击中一枪，那么是师傅打中的，还是徒弟打中的？
（2）一个袋子中总共有黑白两种颜色100个球，其中一种颜色90个，随机取出一个球，发现是黑球。那么是黑色球90个？还是白色球90个？

对于第（1）个问题，由于师傅的技术一般比徒弟高，因此我们会猜测兔子是师傅打中的。对于第（2）个问题，对于颜色有90个的球，我们抽中它的概率更大，因此当抽中为黑色球时，我们便会认为90个的是黑色球。
  对于以上两个例子可以看出，我们在进行猜测时，往往认为：概率最大的事件，最可能发生，**因此在一次试验中就出现的事件应当具有较大的概率。**

#### 2. 极大似然原理及数学表示

  **极大似然原理**是指：若一次试验有 n个可能结果 $A_1,A_2,...,A_n$ ，现在我们做一次试验，试验的结果为 $A_i$ ，那么我们就可以认为事件 $A_i$在这个 n 个可能结果中出现的概率最大。
  **极大似然估计**是指：在一次抽样中，样本出现的概率是关于参数 θ的函数，若在一些试验中，得到观测值 $x_1,x_2,...,x_n$ ，则我们可以选取 $θ^(x_1,x_2,..,x_n)$ 作为 θ 的估计值，使得当 $θ=θ^(x_1,x_2,..,x_n)$时，样本出现的概率最大。而极大似然估计就是要求解出参数 θ 的估计值。可采用**极大似然估计法**。

#### 3. 极大似然估计法(Maximum Likelihood Estimation,MLE)

  （1）若总体 X 为离散型
    假设分布律为 P{X=x}=p(x;θ) ，θ 为待估计参数，p(x;θ) 表示估计参数为 θ 时，发生 x 的概率。
    那么当样本值为： $x_1,x_2,...,x_n​$ 时，

​                             $L(θ)=L(x_1,x_2,...,x_n;θ)=\prod_{i=1}^np(x_i;θ)​$

    其中L(θ)称为样本的似然函数。

    若满足：

$L(x_1,x_2,...,x_n;θ)=maxθL(x_1,x_2,...,x_n;θ)​$

    也就是说，当参数θ=θ^时，似然函数可以取最大值，那么θ^就叫做参数θ的极大似然估计值。

  （2）若总体X为连续型

    假设概率密度为f(x;θ)，θ为待估计参数。

    那么当样本值为：$x_1,x_2,...,x_n​$时，

$L(θ)=L(x_1,x_2,...,x_n;θ)=∏_{i=1}^nf(x_i;θ)​$

    其中L(θ)称为样本的似然函数。

    若满足：

$L(x_1,x_2,...,x_n;θ^)=maxθL(x_1,x_2,...,x_n;θ)​$

    也就是说，当参数θ=θ^时，似然函数可以取最大值，那么θ^就叫做参数θ的极大似然估计值。







