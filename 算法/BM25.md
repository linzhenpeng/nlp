https://www.jianshu.com/p/53e379483f3e

https://blog.csdn.net/weixin_41090915/article/details/79053584

bm25 是一种用来评价搜索词和文档之间相关性的算法，它是一种基于**概率检索模型**提出的算法.

一、**优缺点**
适用于：在文档包含查询词的情况下，或者说查询词精确命中文档的前提下，如何计算相似度，如何对内容进行排序。

不适用于：基于传统检索模型的方法会存在一个固有缺陷，就是检索模型只能处理 Query 与 Document 有重合词的情况，传统检索模型无法处理词语的语义相关性。

白话举例：提出一个query：当下最火的女网红是谁？ 
在Document集合中document1的内容为：[当下最火的男明星为鹿晗]； 
document2的内容为：[女网红能火的只是一小部分]。 
显然document1和document2中都包含[火]、[当下]、[网红]等词语。但是document3的内容可能是：[如今最众所周知的网络女主播是周二柯]。很显然与当前Query能最好匹配的应该是document3，可是document3中却没有一个词是与query中的词相同的（即上文所说的没有“精确命中”），此时就无法应用BM25检索模型。


最后对于每个单词的分数我们做一个求和，就得到了query和文档之间的分数。

二、算法核心
BM25算法是一种常见用来做相关度打分的公式，思路比较简单，主要就是计算一个query里面所有词$q_1,q_2...q_n$和文档的相关度，然后再把分数做累加操作。公式如下： 

$$
Score(Q,d)=\sum_i^nW_i⋅R(q_i,d)
$$
其中$R(q_i,d)$是查询语句query中每个词$q_i$和文档$d$的相关度值，$W_i$是该词的权重，最后将所有词的$W_i∗R(q_i,d)$相加。
$W_i$一般情况下为IDF(InverseDocumentFrequency)值，即逆向文档频率，公式如下： 
$$
IDF(q_i)=log\frac{N+0.5}{n(q_i)+0.5}​
$$
其中N是文档总数，$n(q_i)​$是包含该词的文档数，0.5是调教系数，避免$n(q_i)=0​$的情况。log函数是为了让IDF的值受N和$n(q_i)​$的影响更加平滑。 
从公式中显然能看出IDF值的含义：即总文档数越大，包含词$q_i​$的文档数越小，则$q_i​$的IDF值越大。 
白话举例就是：比如我们有1万篇文档，而单词basketball,Kobe Bryant几乎只在和体育运动有关的文档中出现，说明这两个词的IDF值比较大，而单词is, are, what几乎在所有文档中都有出现，那么这几个单词的IDF值就非常小。

解决了$W_i$，现在再来解决$R(q_i,d)$。$R(q_i,d)$公式如下： 

$$
R(q_i,d)=\frac{f_i⋅(k_1+1)}{f_i+K}⋅\frac{qf_i⋅(k_2+1)}{qf_i+k_2}
$$
其中$k_1,k_2,b$都是调节因子，一般$k_1=1,k_2=1,b=0.75$。 
式中$qf_i$为词$q_i$在**查询语句query中的出现频率**，$f_i$为$q_i$在**文档d中的出现频率**。由于绝大多数情况下一条简短的查询语句query中，词$qi$只会出现一次，即$qf_i=1​$，因此公式可化简为： 
$$
R(q_i,d)=\frac{f_i⋅(k_1+1)}{f_i+K}
$$
其中
$$
K=k_1⋅(1−b+b⋅\frac{dl}{avgdl})
$$
$dl$为文档d的长度，$avgdl$为所有文档的平均长度。意即该文档d的长度和平均长度比越大，则K越大，则相关度$R(q_i,d)$越小,b为调节因子，b越大，则文档长度所占的影响因素越大，反之则越小。 
白话举例就是：一个query为：诸葛亮在哪里去世的？ 
document1的内容为：诸葛亮在五丈原积劳成疾，最终去世； 
document2的内容为：司马懿与诸葛亮多次在五丈原交锋； 
而document3为一整本中国历史书的内容。 
显然document3中肯定包含了大量[诸葛亮]、[哪里]、[去世]这些词语，可是由于document3文档长度太大，所以K非常大，所以和query中每个词$q_i$的相关度$R(q_i,d)​$非常小。
综上所述，可将BM25相关度打分算法的公式整理为： 

$$
Score(Q,d)=\sum_i^nIDF(q_i)⋅\frac{f_i⋅(k_1+1)}{f_i+k_1⋅(1−b+b⋅\frac{dl}{avgdl})}
$$

```python
def init(self):
        for doc in self.docs:
            tmp = {}
            for word in doc:
                if not word in tmp:
                    tmp[word] = 0
                tmp[word] += 1
            self.f.append(tmp)
            for k, v in tmp.items():
                if k not in self.df:
                    self.df[k] = 0
                self.df[k] += 1
        for k, v in self.df.items():
            self.idf[k] = math.log(self.D-v+0.5)-math.log(v+0.5)
##计算出每个词的频次来，然后利用之前的公式计算idf
 
def sim(self, doc, index):
        score = 0
        for word in doc:
            if word not in self.f[index]:
                continue
            d = len(self.docs[index])
            score += 
            (self.idf[word]*self.f[index][word]*(self.k1+1)/      (self.f[index][word]+self.k1*(1-self.b+self.b*d/ self.avgdl)))
 
            #利用之前的计算bm25的公式计算。
        return score
def simall(self, doc):#该doc（也就是之前提到的搜索预料）与预保存的文档）逐个比较相似情况。返回相似值的列别。
        scores = []
        for index in range(self.D):
            score = self.sim(doc, index)
            scores.append(score)
        return scores
```

