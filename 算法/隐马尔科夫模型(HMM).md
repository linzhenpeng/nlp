https://www.cnblogs.com/pinard/p/6972299.html

https://www.cnblogs.com/jiangxinyang/p/9279711.html

https://zhuanlan.zhihu.com/p/26811689

https://blog.csdn.net/maverick17/article/details/79574917

https://blog.csdn.net/DeepOscar/article/details/81036635

https://blog.csdn.net/baimafujinji/article/details/51285082

## 马尔科夫模型

#### 马尔可夫过程

马尔可夫过程（Markov process）是一类[随机过程](http://baike.baidu.com/subview/18964/11233746.htm)。它的原始模型[马尔可夫链](http://baike.baidu.com/view/340221.htm)，由俄国数学家A.A.[马尔可夫](http://baike.baidu.com/view/153690.htm)于1907年提出。该过程具有如下特性：在已知目前状态（现在）的条件下，它未来的演变（将来）不依赖于它以往的演变 (过去 )。例如森林中动物头数的变化构成——马尔可夫过程。在现实世界中，有很多过程都是马尔可夫过程，如液体中微粒所作的[布朗运动](http://baike.baidu.com/view/17875.htm)、传染病受感染的人数、车站的候车人数等，都可视为马尔可夫过程。

在马尔可夫性的定义中，"现在"是指固定的时刻，但实际问题中常需把马尔可夫性中的“现在”这个时刻概念推广为停时（见随机过程）。例如考察从圆心出发的平面上的布朗运动，如果要研究首次到达圆周的时刻 τ以前的事件和以后的事件的条件独立性，这里τ为停时，并且认为τ是“现在”。如果把“现在”推广为停时情形的“现在”，在已知“现在”的条件下，“将来”与“过去”无关，这种特性就叫强马尔可夫性。具有这种性质的马尔可夫过程叫强马尔可夫过程。在相当一段时间内，不少人认为马尔可夫过程必然是强马尔可夫过程。首次提出对强马尔可夫性需要严格证明的是J.L.[杜布](http://baike.baidu.com/view/580182.htm)。直到1956年，才有人找到马尔可夫过程不是强马尔可夫过程的例子。马尔可夫过程理论的进一步发展表明，强马尔可夫过程才是马尔可夫过程真正研究的对象。

**一个马尔科夫过程就是指过程中的每个状态的转移只依赖于之前的 n个状态，这个过程被称为1个 n阶的模型，其中 n是影响转移状态的数目。最简单的马尔科夫过程就是一阶过程，每一个状态的转移只依赖于其之前的那一个状态。**

#### 马尔科夫模型

**马尔科夫模型（VMM）**：它描述了一类重要的随机过程。

​       一个系统有有限个状态集$S = {s_1,s_2,s_N}$，随时间推移，该系统将同某一状态转移到另一状态。$Q=(s_1,s_2,,,s_N)​$为一随机变量序列，随机变量取值为状态集S中的一个状态，设时间t时状态为qt。

对系统的描述通常是给出当前时刻t的状态与其前面所有状态的关系：当前时刻 t 处于状态sj的概率取决于其在时间1,2，···，t-1时刻的状态，该概率为

​         $P（q_t = s_j | q_{t-1} = s_i,q_{t-2} = s_k ,···）.$

　　特定条件下，系统在当前时间t的状态只与t-1的状态相关，即：

​         $P（q_t = s_j | q_{t-1} = s_i,q_{t-2} = s_k ,···）= P(q_t = s_j | q_{t-1} = s_i)$，该系统构成一个离散的一阶马尔科夫链。

　　进一步，如果只考虑上式独立于时间t的随机过程（即与时间t的具体大小无关，亦即下面说到的状态转移矩阵不随时间变化）：

　　 P(qt = sj | qt-1 = si) = aij，1<= i,j <=N，该随机模型称为马尔可夫模型。其中状态转移概率必须满足：aij>= 0，ai1 + ai2+...+ajN = 1 。

有N个状态的一阶马尔科夫过程有N2次状态转移，它们可以表示成一个状态转移矩阵。

　　马尔科夫模型可以视为一个随机的有限状态机。一个马尔科夫链的状态序列的概率可以通过状态转移矩阵上的状态转移概率计算。

## HMM的基本模型

隐马尔科夫模型是  指模型所经过的状态序列（模型的状态转换过程是不可观察的，是隐蔽的），只知道状态的随机函数。

隐马尔科夫链，这是一个特别常见的模型，在自然语言处理中的应用也非常多。

**常见的应用比如分词，词性标注，命名实体识别等问题序列标注问题均可使用隐马尔科夫模型.**

 #### **隐马尔科夫模型定义**

隐马尔可夫模型是关于**时序**的概率模型，描述由一个**隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。**

**隐藏的马尔可夫链随机生成的状态的序列，称为状态序列（state sequence);每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence)。**

序列的每一个位置又可以看作是一个**时刻**。

下面我们引入一些符号来表示这些定义：

设Q是所有可能的状态的集合，V是所有可能的观测的集合。

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-06a3db8c77a09b7dd844eddba5d1f27e_hd.png)



**其中，N是可能的状态数，M是可能的观测数。**

**状态q是不可见的，观测v是可见的。**

> **应用到词性标注中，**v代表词语，是可以观察到的。q代表我们要预测的词性（一个词可能对应多个词性）是隐含状态。
>
> **应用到分词中，**v代表词语，是可以观察的。q代表我们的标签（B,E这些标签，代表一个词语的开始，或者中间等等）
>
> **应用到命名实体识别中，**v代表词语，是可以观察的。q代表我们的标签（标签代表着地点词，时间词这些）

**I是长度为T的状态序列，O是对应的观测序列。**

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-07c3af4a16a5babbdad861edd7b539a0_hd.png)

我们可以看做是给定了一个词（O）+词性（I）的训练集。或者一个词(O)+分词标签（I）的训练集....有了训练数据，那么再加上训练算法则很多问题也就可以解决了，问题后面慢慢道来~

我们继续定义A为状态转移概率矩阵：

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-05d40f68c8dc780bbc1d7315e131d550_hd.png)

其中，

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-7d8cff2ce444c634e7d9db91aab63491_hd.png)

是在时刻t处于状态qi的条件下在时刻t+1转移到状态qj的概率。

**B是观测概率矩阵:**

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-ccf0fa4a8ccb3e9eeb0bc1ec26540eeb_hd.png)

其中，

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-5dfbce14b118b8cdb942a1ec3cab6b96_hd.png)

是在时刻t处于状态qj的条件下生成观测vk的概率（**也就是所谓的“发射概率”**）。

**所以我们在其它资料中，常见到的生成概率与发射概率其实是一个概念。**

**π是初始状态概率向量：**

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-210bc9b422d33213a6f7711c671cc559_hd.png)

其中，

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-cda60d7c046fd267f950d6e99047de81_hd.png)

隐马尔可夫模型由初始状态概率向量π、状态转移概率矩阵A和观测概率矩阵B决定。π和A决定状态序列，B决定观测序列。因此，隐马尔可夫模型可以用三元符号表示，即

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\v2-4a86b85795372150dc36a28b10802a76_hd.png)

这三个元素被称为隐马尔可夫模型的三要素。

**如果加上一个具体的状态集合Q和观测序列V，构成了HMM的五元组，这也是隐马尔科夫模型的所有组成部分。**

#### 实例

示例说明

假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。 

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\20160901150019846.jpg)



假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。

例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4

这串数字叫做**可见状态链**（对应上面公式的观察到的结果为 Y）。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串**隐含状态链**（对应上面公式的隐藏的为 X）。

在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8

一般来说，

HMM中说到的马尔可夫链其实是指隐含状态链，隐含状态（骰子）之间存在转换概率（transition probability）。

在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1，这样就是一个新的HMM，一般情况权重设定也确实是不一样的。

同样的，

尽管**可见状态**之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做**输出概率**（emission probability）。

就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。

![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\20160901150419179.jpg)

而三个骰子之间也是可以相互转换的，其转换关系示意图如下所示。



![](F:\git\nlp\算法\images\隐马尔科夫模型(HMM)\20160901150428725.jpg)



其实对于HMM来说，如果提前知道所有隐含状态之间的**转换概率**和所有隐含状态到所有可见状态之间的**输出概率**，做模拟是相当容易的。



### HMM的三个基本问题

但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。

如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。这应该如何做呢？下面就来说明。

这里就要顺带着说明与HMM模型相关的算法了，算法分为三类，分别对应着解决三种问题：



1. **评估问题**: 给定观察序列O = O1O2···OT和模型μ，如何快速的计算出给定模型μ情况下，观察序列O的概率，即P(O|μ)？
   知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道**掷出这个结果的概率**。

   这个问题看似意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率，问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子给换了。通常利用**前向算法**，分别计算每个产生给定观测序列的概率，然后从中选出最优的HMM模型。

   https://www.cnblogs.com/pinard/p/6955871.html

   

2. **解码问题**:给定观察序列O = O1O2···OT和模型μ，如何快速选择在一定意义下“最优“的状态序列Q = q1q2···qT,使得该状态序列“最好的解释”观察序列？分词,命名实体识别 都是已知观测序列求 状态序列
   知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我**想知道每次掷出来的都是哪种骰子（隐含状态链）**。

   https://www.cnblogs.com/pinard/p/6991852.html

3. **学习问题**:给定观察序列O = O1O2···OT，如何根据最大似然估计求模型的参数值？即如何调节模型μ的参数是的P(O|μ)最大？
   知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我**想反推出每种骰子是什么（转换概率）**。

   这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是**建模**的一个必要步骤，通常使用Baum-Welch算法解决。

   https://www.cnblogs.com/pinard/p/6972299.html



 　1. 对于一个观察序列匹配最可能的系统——评估，使用前向算法（forward algorithm）解决；
  　2. 对于已生成的一个观察序列，确定最可能的隐藏状态序列——解码，使用Viterbi 算法（Viterbi algorithm）解决；
  　3. 对于已生成的观察序列，决定最可能的模型参数——学习，使用前向-后向算法（forward-backward algorithm）解决。











